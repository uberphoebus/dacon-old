{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db1b18f2",
   "metadata": {},
   "source": [
    "### 최종 결과물 : Catboost + NN Ensemble (Public : 5.226109375, Private : 8.73413)\n",
    "### 목차\n",
    "#### 0. Library\n",
    "#### 1. Feature Engineering\n",
    "##### 1-1) Catboost Data\n",
    "- 데이터 원본 => 최초데이터 형태로 복구\n",
    "- Mean Target Encoding\n",
    "##### 1-2) NN Data\n",
    "- Step1\n",
    "    - 데이터 원본\n",
    "    - 격자공간 정보 기반 외부 데이터 병합\n",
    "- Step2\n",
    "    - Step1 Data -> 일부 Feature 제외 Onehot Encoding -> Deep Auto Encoder\n",
    "    - Step1 Data -> Mean Target Encoding -> Polynomial Features\n",
    "#### 2. Catboost Modeling\n",
    "- Catboost Data\n",
    "- Catboost Regressor에 통채로 넣어 예측\n",
    "#### 3. NN Modeling\n",
    "- NN Data\n",
    "- Fully Connected Netowrk 구성\n",
    "#### 4. Ensemble & Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4ad9a7",
   "metadata": {},
   "source": [
    "## 0. Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1c3043c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading & Save Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "## print log\n",
    "import traceback\n",
    "\n",
    "## Feature Generation\n",
    "\n",
    "## NN Modeling & DAE\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Others\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b5a8245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_warning_messages(e, step) : \n",
    "    \"\"\"\n",
    "    traceback 모듈을 사용해 어디서 에러가 발생했는지 출력\n",
    "    매개변수 :\n",
    "        e : error 메세지\n",
    "        step : 현재 step -> 어디서 에러가 발생했는지 확인하기 위해\n",
    "    출력 : \n",
    "        몇번째 step에서 어떤 error가 발생했는지 출력\n",
    "    \"\"\"\n",
    "\n",
    "    log =''\n",
    "    log += '='*10+'\\n'\n",
    "    log += f\"[ERROR : {step}] Unexpected error : {e}\" + '\\n'\n",
    "    log += traceback.format_exc() +'\\n'\n",
    "    log += '='*10+'\\n'\n",
    "    print(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aa4a05",
   "metadata": {},
   "source": [
    "## 1. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b961c0fc",
   "metadata": {},
   "source": [
    "### 1-0) Function for both data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e8aa0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path): \n",
    "    \"\"\"\n",
    "    데이터를 불러와 반환하는 작업\n",
    "    매개변수 :\n",
    "        path : 데이터가 담겨져있는 변수\n",
    "            list -> 원본데이터로 판단\n",
    "            str  -> pickle 데이터로 판단\n",
    "    반환값 : \n",
    "        train, test\n",
    "    \"\"\"\n",
    "    if type(path) == list : \n",
    "        train_path = path[0]; test_path = path[1]\n",
    "        train = pd.read_csv(train_path, encoding = 'cp949')\n",
    "        test = pd.read_csv(test_path, encoding = 'cp949')\n",
    "\n",
    "        kor_columns = ['index', '송장인번호', '수하인번호', '카테고리_대', '카테고리_중']\n",
    "        train.columns = kor_columns + ['운송장_건수']\n",
    "        test.columns = kor_columns\n",
    "        return train, test\n",
    "    else : \n",
    "        import pickle\n",
    "        with open(path, 'rb') as f : \n",
    "            train, test, describe = pickle.load(f)\n",
    "        f.close()\n",
    "        \n",
    "        return  train, test, describe\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc7e0cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_target_encoding(train, test, columns) : \n",
    "    \"\"\"\n",
    "    Mean Target Encoding : train 데이터의 값을 기준으로, 종속 변수에 대한 평균값으로 Encoding 하는 방법\n",
    "    장점 : Label Encoding의 수치 변환에 대한 우려 X, Onehot Encoding의 차원의 저주에 대한 우려 X\n",
    "    단점 : 실서비스 반영시 주기적인 업데이트 필요 , Data Leakage 발생하지 않도록 조심, Train Data Overfitting 가능성 존재\n",
    "\n",
    "    매개변수 : \n",
    "        train    : 변환 작업을 위해 사용\n",
    "        test     : 변환 작업을 위해 사용\n",
    "        columns  : 변환 대상인 열들을 지정\n",
    "    반환값 : \n",
    "        train : train 원본 데이터에 axis=1 방향으로 mean target encoding을 병합한 데이터\n",
    "        test  : test  원본 데이터에 axis=1 방향으로 mean target encoding을 병합한 데이터\n",
    "    \"\"\"\n",
    "    med = train['운송장_건수'].median() # 운송장_건수에는 이상치가 상당히 존재하기 때문에, median으로 채워준다.\n",
    "    for column in columns :  \n",
    "        new_column = 'Mean_' + column\n",
    "        dic = train.groupby(column)['운송장_건수'].agg('mean').to_dict()\n",
    "        train[new_column] = train[column].map(dic)\n",
    "        test[new_column] = test[column].map(dic)\n",
    "        test[new_column] = test[new_column].fillna(med)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f1beaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(train, test, describe, path) : \n",
    "    \"\"\"\n",
    "    데이터를 pickle형태로 저장한 후, 추후 모델링 과정에서 불러올 예정\n",
    "    매개 변수 : \n",
    "        train    : 저장할 train data\n",
    "        test     : 저장할 test data\n",
    "        describe : 메타정보\n",
    "    \"\"\"\n",
    "\n",
    "    import pickle\n",
    "    if path in os.listdir('./data') : \n",
    "        os.remove(path)\n",
    "        print('Update Data')\n",
    "    \n",
    "    with open(path, 'wb') as f :\n",
    "        pickle.dump([train, test, describe], f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c97f59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prediction(sample, submission_path, pred, y_train) : \n",
    "    \"\"\"\n",
    "    최종 결과물을 저장한다.\n",
    "    매개변수 : \n",
    "        sample              : 저장을 위해 사용될 sample data\n",
    "        submission_path     : 저장될 위치\n",
    "        pred                : 저장할 예측 값\n",
    "        y_train             : post pre-processing을 위해 사용\n",
    "    \"\"\"\n",
    "\n",
    "    if 'submissions' not in os.listdir(): \n",
    "        os.makedirs('submissions')\n",
    "        print('Made paths in submissions')\n",
    "    minimum = y_train.min()\n",
    "    pred[pred < minimum ] = minimum\n",
    "    display(pd.DataFrame(pred).describe())\n",
    "    sample['INVC_CONT'] = pred\n",
    "    sample.to_csv(submission_path, index=False)\n",
    "    print(f'=== Prediction Saved At : {submission_path} ===')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ab8951",
   "metadata": {},
   "source": [
    "### 1-1 ) Catboost Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f38a2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_clear_data(train, test) : \n",
    "    \"\"\"\n",
    "    원본 데이터 형태로 처리한다.\n",
    "    \"\"\"\n",
    "    train[['송장인번호','수하인번호']] = train[['송장인번호','수하인번호']].applymap(lambda x : str(x)[:5]+'0'*7)\n",
    "    test[['송장인번호','수하인번호']] = test[['송장인번호','수하인번호']].applymap(lambda x : str(x)[:5]+'0'*7)\n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcbfbc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __catboost_engineering_main__(save_path) : \n",
    "    try : \n",
    "        step = 1\n",
    "        # 데이터 불러오기\n",
    "        try : \n",
    "            train_path = './data/train_df.csv'\n",
    "            test_path = './data/test_df.csv'\n",
    "            train, test = load_data([train_path, test_path])\n",
    "    \n",
    "            step += 1\n",
    "        except Exception as e :\n",
    "            return_warning_messages(e, step = step)\n",
    "            return 1\n",
    "        \n",
    "        # 초기 데이터 상태로 만들기\n",
    "        try : \n",
    "            train, test = cat_clear_data(train, test)\n",
    "            step += 1\n",
    "        except Exception as e :\n",
    "            return_warning_messages(e, step = step)\n",
    "            return 1\n",
    "        \n",
    "        # Mean target encoding 적용하기\n",
    "        try : \n",
    "            mean_target_encoding_columns = list(train.drop(columns = ['index','운송장_건수']).columns)\n",
    "            train, test = mean_target_encoding(train, test, mean_target_encoding_columns)\n",
    "            step += 1\n",
    "        except Exception as e :\n",
    "            return_warning_messages(e, step = step)\n",
    "            return 1\n",
    "        \n",
    "        try : \n",
    "            describe = 'Data Type : Pandas DataFrame\\nRaw Data + Mean Target Encoding'\n",
    "            save_data(train, test, describe, path = save_path) \n",
    "        except Exception as e :\n",
    "            return_warning_messages(e, step = step)\n",
    "            return 1\n",
    "        \n",
    "    except Exception as e: \n",
    "        return_warning_messages(e, step = 0)\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c35a1b",
   "metadata": {},
   "source": [
    "### 1-2) NN Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fc9bf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_clear_data(train, test) :\n",
    "    \"\"\"\n",
    "    국토연구원에서 제공하는 50m 격자단위 데이터를 결합합니다.\n",
    "    데이터 링크 : https://www.bigdata-region.kr/#/dataset/0ad3c882-f7ee-4faf-970d-00c53cb65a84\n",
    "\n",
    "    매개변수 : \n",
    "        train : 원본 데이터에 병합해서 사용할 예정\n",
    "        test  : 원본 데이터에 병합해서 사용할 에정\n",
    "    \"\"\"\n",
    "    from glob import glob\n",
    "    from datetime import datetime\n",
    "\n",
    "    if 'NN_train.csv' not in os.listdir('./data/') : \n",
    "        print('=== Installing New Data Initiated === / ', datetime.today())\n",
    "        if \"50m\" not in os.listdir('./data/') : \n",
    "            print('50M 격자공간 데이터가 존재하지 않습니다.\\n 데이터를 다운받아주세요')\n",
    "        grid_50m = pd.DataFrame()\n",
    "        for path in glob('./data/50m/*.csv') :\n",
    "            grid_50m = pd.concat([grid_50m, pd.read_csv(path)], axis = 0).astype('str')\n",
    "        grid_50m_name = dict(zip(grid_50m.격자공간고유번호, grid_50m.격자공간명))\n",
    "        grid_50m_code = dict(zip(grid_50m.격자공간고유번호, grid_50m.시군구코드))\n",
    "        grid_50m_sigu = dict(zip(grid_50m.격자공간고유번호, grid_50m.시군구명))\n",
    "        train['송장인_name'] = train['송장인번호'].astype('str').map(grid_50m_name)\n",
    "        train['송장인_code'] = train['송장인번호'].astype('str').map(grid_50m_code)\n",
    "        train['송장인_sigu'] = train['송장인번호'].astype('str').map(grid_50m_sigu)\n",
    "        train['수하인_name'] = train['수하인번호'].astype('str').map(grid_50m_name)\n",
    "        train['수하인_code'] = train['수하인번호'].astype('str').map(grid_50m_code)\n",
    "        train['수하인_sigu'] = train['수하인번호'].astype('str').map(grid_50m_sigu)\n",
    "        test['송장인_name'] = test['송장인번호'].astype('str').map(grid_50m_name)\n",
    "        test['송장인_code'] = test['송장인번호'].astype('str').map(grid_50m_code)\n",
    "        test['송장인_sigu'] = test['송장인번호'].astype('str').map(grid_50m_sigu)\n",
    "        test['수하인_name'] = test['수하인번호'].astype('str').map(grid_50m_name)\n",
    "        test['수하인_code'] = test['수하인번호'].astype('str').map(grid_50m_code)\n",
    "        test['수하인_sigu'] = test['수하인번호'].astype('str').map(grid_50m_sigu)\n",
    "\n",
    "        train.to_csv('./data/NN_train.csv', index=False)\n",
    "        test.to_csv('./data/NN_test.csv', index= False)\n",
    "        print('=== Installing New Data Finishied === / ', datetime.today())\n",
    "    else : \n",
    "        print('=== NN Data Exist ===')\n",
    "    train = pd.read_csv('./data/NN_train.csv')\n",
    "    test = pd.read_csv('./data/NN_test.csv')\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca101d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_features(train, test, columns) : \n",
    "    \"\"\"\n",
    "    Sklearn.preprocessing Polynomial Features : row단위로 데이터를 곱하거나 제곱해 Feature Generation 하는 방법\n",
    "    Reference : https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html\n",
    "    \n",
    "    매개변수 : \n",
    "        train    : 변환 작업을 위해 사용\n",
    "        test     : 변환 작업을 위해 사용\n",
    "        columns  : 변환 대상인 열들을 지정\n",
    "    반환값 : \n",
    "        train_poly : Polynomial Features를 적용한 값\n",
    "        test_poly : Polynomial Feature를 적용한 값\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    poly = PolynomialFeatures(degree=2)\n",
    "    poly.fit(train[columns])\n",
    "    new_columns = poly.get_feature_names(columns)\n",
    "    train_poly = pd.DataFrame(poly.transform(train[columns]), columns = new_columns)\n",
    "    test_poly = pd.DataFrame(poly.fit_transform(test[columns]), columns = new_columns)\n",
    "    return train_poly, test_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2ae50bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encoding(train, test ,columns, need_df = False) : \n",
    "    \"\"\"\n",
    "    매개변수 : \n",
    "        train    : 변환 작업을 위해 사용\n",
    "        test     : 변환 작업을 위해 사용\n",
    "        columns  : 변환 대상인 열들을 지정\n",
    "        need_df  : DataFrame 형태로 반환할지, numpy array 형태로 반환할지 지정\n",
    "    반환값 : \n",
    "        train_onehot : OneHotEncoding 된 train 값\n",
    "        test_onehot  : OneHotEncoding 된 test 값\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "    train_onehot = encoder.fit_transform(train[columns]).toarray()\n",
    "    test_onehot  = encoder.transform(test[columns]).toarray()\n",
    "\n",
    "    if need_df : \n",
    "        onehot_columns = encoder.get_feature_names(columns)\n",
    "        train_onehot = pd.DataFrame(train_onehot, columns = onehot_columns)\n",
    "        test_onehot = pd.DataFrame(test_onehot, columns = onehot_columns)\n",
    "    return train_onehot, test_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e637dcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_auto_model(input_dim) : \n",
    "    def hidden(x, dim_size) : \n",
    "        x = Dense(dim_size, activity_regularizer=tf.keras.regularizers.l1(1e-6), kernel_initializer = 'he_normal')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = ReLU()(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        return x\n",
    "    hid_dim1 = input_dim // 2\n",
    "    hid_dim2 = input_dim // 4\n",
    "    hid_dim3 = input_dim // 8\n",
    "\n",
    "    auto_in = Input(shape = (input_dim,))\n",
    "    h1 = hidden(auto_in, hid_dim1)\n",
    "    h2 = hidden(h1, hid_dim1)\n",
    "    h3 = hidden(h2, hid_dim2)\n",
    "    h4 = hidden(h3, hid_dim3)\n",
    "    h5 = hidden(h4, hid_dim2)\n",
    "    h6 = hidden(h5, hid_dim1)\n",
    "    auto_out = Dense(input_dim, activation = 'sigmoid')(h6)\n",
    "    auto_model = Model(auto_in, auto_out)\n",
    "    auto_model.compile(optimizer = 'adam', loss = 'mse')\n",
    "    return auto_model\n",
    "\n",
    "def get_callbacks(model_save_path, patience) : \n",
    "    \"\"\"\n",
    "    매개변수 : \n",
    "        model_save_path  : Auto encoder 모델 저장 경로\n",
    "        patience         : Early Stopping에 적용 / \n",
    "    반환값 : \n",
    "        callbacks : 모델 학습에 필요한 callback 함수들\n",
    "    \"\"\"\n",
    "    from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "    if 'models' not in os.listdir() : \n",
    "        os.makedirs('models')\n",
    "    if model_save_path.replace('./models/','') in os.listdir('./models/') :\n",
    "        os.remove(model_save_path)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=patience//3,verbose = 0, min_delta=1e-7)\n",
    "    early = EarlyStopping(monitor = 'val_loss', patience = patience, verbose = 0)\n",
    "    mck = ModelCheckpoint(filepath=model_save_path,  monitor='val_loss', save_best_only=True, verbose = 0, model = 'min')\n",
    "    callbacks = [reduce_lr, early, mck]\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca4c7dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_encoded_data(train_onehot, test_onehot, model_save_path, patience) : \n",
    "    from sklearn.model_selection import KFold\n",
    "    from datetime import datetime\n",
    "    print('=== AUTO ENCODING STARTED ===  /', datetime.today())\n",
    "    callbacks = get_callbacks(model_save_path, patience)\n",
    "    new_trains = [] ; new_tests = []\n",
    "    kf = KFold(n_splits=5, shuffle = True)\n",
    "    loss = 0\n",
    "    for train_idx, valid_idx in kf.split(np.arange(train_onehot.shape[0])) :\n",
    "        tr_onehot = train_onehot[train_idx]; val_onehot = train_onehot[valid_idx]\n",
    "        auto_model = create_auto_model(tr_onehot.shape[1])\n",
    "        history = auto_model.fit(tr_onehot, tr_onehot,\n",
    "                    epochs = 20000,\n",
    "                    batch_size = tr_onehot.shape[0]//10,\n",
    "                    validation_data=(val_onehot, val_onehot),\n",
    "                    callbacks = callbacks,\n",
    "                    verbose = 0\n",
    "                    )\n",
    "        auto_model.load_weights(model_save_path)\n",
    "        new_trains.append(auto_model.predict(train_onehot))\n",
    "        new_tests.append(auto_model.predict(test_onehot))\n",
    "        loss += min(history.history['val_loss'])\n",
    "    train_AE = np.mean(new_trains, axis = 0)\n",
    "    test_AE = np.mean(new_tests, axis = 0)\n",
    "\n",
    "    print(train_AE.shape, test_AE.shape, loss / 5)\n",
    "    print('=== AUTO ENCODING ENDED ===  /', datetime.today())\n",
    "    return train_AE, test_AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86f29543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __NN_engineering_main__(save_path, need_AE_update) : \n",
    "    try : \n",
    "        step = 1\n",
    "        try :  # STEP 1 : Reset Data\n",
    "            train_path = './data/train_df.csv'\n",
    "            test_path = './data/test_df.csv'\n",
    "            train, test = load_data([train_path, test_path])\n",
    "            step += 1\n",
    "        except Exception as e :\n",
    "            return_warning_messages(e, step = step)\n",
    "            return 1\n",
    "\n",
    "        try : # STEP 2 : Merge Grid Data\n",
    "            train, test = NN_clear_data(train, test)\n",
    "            step += 1\n",
    "        except Exception as e :\n",
    "            return_warning_messages(e, step = step)\n",
    "            return 1\n",
    "\n",
    "        try : # STEP 3 : Mean Target Encoding\n",
    "            mean_target_encoding_columns = ['송장인번호', '카테고리_대', '카테고리_중', \n",
    "                            '송장인_name' ,'송장인_sigu', '송장인_code',\n",
    "                            '수하인_sigu', '수하인_code']\n",
    "            train, test = mean_target_encoding(train, test, mean_target_encoding_columns)\n",
    "            step += 1\n",
    "        except Exception as e :\n",
    "            return_warning_messages(e, step = step)\n",
    "            return 1\n",
    "        \n",
    "        try : # STEP 4 : Polynomial Features\n",
    "            polynomial_features_columns = list(train.filter(like = 'Mean_').columns)\n",
    "            train_poly, test_poly = polynomial_features(train, test, polynomial_features_columns)\n",
    "            step += 1\n",
    "        except Exception as e :\n",
    "            return_warning_messages(e, step = step)\n",
    "            return 1\n",
    "        \n",
    "        if need_AE_update : \n",
    "            try : # STEP 5 : Onehot Encoding\n",
    "                onehot_encoding_columns = ['송장인번호', '카테고리_대', '카테고리_중', \n",
    "                            '송장인_name' , '송장인_sigu','송장인_code',\n",
    "                            '수하인_sigu', '수하인_code']\n",
    "                train_onehot, test_onehot = onehot_encoding(train, test, onehot_encoding_columns)\n",
    "                step += 1\n",
    "            except Exception as e :\n",
    "                return_warning_messages(e, step = step)\n",
    "                return 1\n",
    "            \n",
    "            try : # STEP 6 : Auto Encoding\n",
    "                model_save_path = './models/AUTO_ENCODER.h5'\n",
    "                patience = 15\n",
    "                train_AE, test_AE = return_encoded_data(train_onehot, test_onehot, model_save_path, patience)\n",
    "                step += 1\n",
    "            except Exception as e :\n",
    "                return_warning_messages(e, step = step)\n",
    "                return 1\n",
    "        else : \n",
    "            [[_,train_AE,_], [_, test_AE], _] = load_data(save_path)\n",
    "\n",
    "        try : # STEP 7 : Save Data\n",
    "            describe = 'Data Type : Numpy Array\\nMean Target Encoding -> Polynomial Features & Onehot Encoding -> Deep Auto Encoding'\n",
    "            train = [train_poly, train_AE, train['운송장_건수'].values]\n",
    "            test  = [test_poly, test_AE]\n",
    "            save_data(train, test, describe, path = save_path) \n",
    "        except Exception as e :\n",
    "            return_warning_messages(e, step = step)\n",
    "            return 1            \n",
    "            \n",
    "\n",
    "    except Exception as e: \n",
    "        return_warning_messages(e, step = 0)\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16593e52",
   "metadata": {},
   "source": [
    "## 2. Catboost Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5abd45fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_catboost_modeling_data(train, test) : \n",
    "    X_train = train.drop(columns = ['index', '운송장_건수'])\n",
    "    y_train = train['운송장_건수'].astype('float32')\n",
    "    X_test  = test.drop(columns = ['index'])\n",
    "    cat_columns = ['송장인번호','수하인번호','카테고리_대','카테고리_중']\n",
    "    sample = pd.read_csv('./data/sample_submission.csv')\n",
    "    return X_train, y_train, X_test, cat_columns, sample\n",
    "\n",
    "def catboost_modeling(X_train, y_train, X_test, cat_columns) : \n",
    "    from datetime import datetime\n",
    "    print('=== Modeling Started   ===  / Initialized at : ', datetime.today())\n",
    "    from catboost import CatBoostRegressor\n",
    "    catboost = CatBoostRegressor()\n",
    "    catboost.fit(X_train, y_train, silent = True, cat_features=cat_columns)\n",
    "    pred = catboost.predict(X_test)\n",
    "    print('=== Modeling Finishied === /  Finishied   at : ', datetime.today())\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11290283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __catboost_main__(data_path, need_update) : \n",
    "    from datetime import datetime\n",
    "    print('!! CATBOOST MAIN STARTED !!  /  ', datetime.today())\n",
    "    print(str(datetime.today()), 'STARTED')\n",
    "    print('')\n",
    "    try : \n",
    "        step = 1\n",
    "        try : # STEP 1 : Load Data\n",
    "            check_path = data_path.replace('./data/','')\n",
    "            if (check_path not in os.listdir('./data/')) or need_update : \n",
    "                print('=== UPDATE DATA ===')\n",
    "                __catboost_engineering_main__(data_path)\n",
    "                print(str(datetime.today()), 'UPDATED')\n",
    "                print('')\n",
    "            train, test, describe = load_data(data_path)\n",
    "            print(describe)\n",
    "        except Exception as e : \n",
    "            return_warning_messages(e, step = step)\n",
    "            return 1\n",
    "\n",
    "        try : # STEP 2 : Set Modeling Data\n",
    "            X_train, y_train, X_test, cat_columns, sample = set_catboost_modeling_data(train, test)\n",
    "            step += 1\n",
    "        except Exception as e : \n",
    "            return_warning_messages(e, step = step)\n",
    "            return 1\n",
    "\n",
    "        try : # STEP 3 : Modeling\n",
    "            prediction = catboost_modeling(X_train, y_train, X_test, cat_columns)\n",
    "            step += 1\n",
    "        except Exception as e : \n",
    "            return_warning_messages(e, step = step)\n",
    "            return 1\n",
    "        \n",
    "        try : # STEP 4 : Save Prediction Data\n",
    "            submission_path = './submissions/CATBOOST.csv'\n",
    "            save_prediction(sample, submission_path, prediction, y_train)\n",
    "        except Exception as e : \n",
    "            return_warning_messages(e, step = 0)\n",
    "            return 1\n",
    "        print('')\n",
    "        print('!! CATBOOST MAIN ENDED !!  /  ', datetime.today())\n",
    "\n",
    "    except Exception as e : \n",
    "        return_warning_messages(e, step = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef6e9644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!! CATBOOST MAIN STARTED !!  /   2021-12-16 20:18:19.984146\n",
      "2021-12-16 20:18:19.984146 STARTED\n",
      "\n",
      "=== UPDATE DATA ===\n",
      "2021-12-16 20:18:20.085660 UPDATED\n",
      "\n",
      "Data Type : Pandas DataFrame\n",
      "Raw Data + Mean Target Encoding\n",
      "=== Modeling Started   ===  / Initialized at :  2021-12-16 20:18:20.103611\n",
      "=== Modeling Finishied === /  Finishied   at :  2021-12-16 20:18:47.269928\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4640.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.807993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.584526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.991876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.355627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.061352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>161.186954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "count  4640.000000\n",
       "mean      4.807993\n",
       "std       3.584526\n",
       "min       3.000000\n",
       "25%       3.991876\n",
       "50%       4.355627\n",
       "75%       5.061352\n",
       "max     161.186954"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Prediction Saved At : ./submissions/CATBOOST.csv ===\n",
      "\n",
      "!! CATBOOST MAIN ENDED !!  /   2021-12-16 20:18:47.298750\n"
     ]
    }
   ],
   "source": [
    "CAT_data_path = './data/CAT_DATA.pkl'\n",
    "need_update = True\n",
    "__catboost_main__(data_path = CAT_data_path, need_update = need_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30629de5",
   "metadata": {},
   "source": [
    "## 3. NN Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed4100dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_NN_modeling_data(train, test) : \n",
    "    X_train = np.concatenate([train[0].to_numpy(), train[1]], axis = 1)\n",
    "    y_train = train[2].astype('float32')\n",
    "    X_test  = np.concatenate([test[0].to_numpy(), test[1]], axis = 1)\n",
    "    print(X_train.shape, y_train.shape, X_test.shape)\n",
    "    sample = pd.read_csv('./data/sample_submission.csv')\n",
    "    return X_train, y_train, X_test, sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4bee6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_modeling(X_train, y_train, X_test, patience, total_epoch) : \n",
    "    def fc_layer(x, unit, dr) : \n",
    "        x = Dense(unit, kernel_initializer = 'he_normal')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU()(x)\n",
    "        x = Dropout(dr)(x)\n",
    "        return x\n",
    "\n",
    "    def rmse(y_true, y_pred):\n",
    "            return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "    def create_model():\n",
    "        inp = Input(shape = X_train.shape[1],)\n",
    "        fc=  fc_layer(inp, 4196, 0.2)\n",
    "        fc=  fc_layer(fc, 1024, 0.2)\n",
    "        fc=  fc_layer(fc, 256, 0.2)\n",
    "        fc = fc_layer(fc, 64, 0.2)\n",
    "        fc = fc_layer(fc, 16, 0.2)\n",
    "        fc = fc_layer(fc, 4, 0.2)\n",
    "        out = Dense(1, activation = 'relu')(fc)\n",
    "\n",
    "        model = Model(inp, out)\n",
    "        optimizer = tf.keras.optimizers.Adam(0.1)\n",
    "        model.compile(optimizer=optimizer, loss=rmse)\n",
    "        return model\n",
    "    def kf_NN_modeling() : \n",
    "        from sklearn.model_selection import KFold\n",
    "        for k in range(5) :\n",
    "            print(f'    ===== FOLD : {k} =====', end = ' ')\n",
    "        print('')\n",
    "        rs = np.random.randint(0,1234578,1)[0]\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state = rs)\n",
    "        fold = 0\n",
    "        preds = []\n",
    "        for train_idx, valid_idx in kf.split(np.arange(X_train.shape[0])) : \n",
    "            X_tr = X_train[train_idx] ; X_val = X_train[valid_idx]\n",
    "            y_tr = y_train[train_idx] ; y_val = y_train[valid_idx]\n",
    "            model_save_path = f'./models/model_{fold}.h5'\n",
    "            callbacks = get_callbacks(model_save_path, patience)\n",
    "            model = create_model()\n",
    "            history = model.fit(X_tr, y_tr,\n",
    "                                epochs = 20000,\n",
    "                                batch_size = X_tr.shape[0]//5,\n",
    "                                validation_data=(X_val, y_val),\n",
    "                                callbacks = callbacks,\n",
    "                                verbose = 0\n",
    "                                )\n",
    "            score = min(history.history['val_loss'])\n",
    "            print('    ',round(score,16), end = '   ')\n",
    "            model.load_weights(model_save_path)\n",
    "            pred = model.predict(X_test).flatten()\n",
    "            preds.append([pred, score])\n",
    "            fold += 1\n",
    "        print('')\n",
    "        return preds\n",
    "    def NN_modeling(epoch) : \n",
    "        print('')\n",
    "        print(f'========== EPOCH : {epoch} ==========')\n",
    "        preds = kf_NN_modeling()\n",
    "        return preds\n",
    "\n",
    "    total_predictions = []\n",
    "    for epoch in range(total_epoch) : \n",
    "        total_predictions.extend(NN_modeling(epoch))\n",
    "    predictions = sorted(total_predictions, key = lambda x: x[1])[:5]\n",
    "    prediction = np.mean([x[0] for x in predictions], axis= 0)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f44146e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __NN_main__(data_path, need_update, need_AE_update) : \n",
    "    from datetime import datetime\n",
    "    print('!! NN MAIN STARTED !!  /  ', datetime.today())\n",
    "    print(str(datetime.today()), 'STARTED')\n",
    "    print('')\n",
    "    try : \n",
    "        step = 1\n",
    "        try : # STEP 1 : Load Data\n",
    "            check_path = data_path.replace('./data/','')\n",
    "            if (check_path not in os.listdir('./data/')) or need_update : \n",
    "                print('=== UPDATE DATA ===')\n",
    "                __NN_engineering_main__(data_path, need_AE_update = need_AE_update)\n",
    "                print(str(datetime.today()), 'UPDATED')\n",
    "                print('')\n",
    "            train, test, describe = load_data(data_path)\n",
    "            print(describe)\n",
    "        except Exception as e : \n",
    "            return_warning_messages(e, step = step)\n",
    "            return 1\n",
    "\n",
    "        try : # STEP 2 : Set Modeling Data\n",
    "            X_train, y_train, X_test, sample = set_NN_modeling_data(train, test)\n",
    "            step += 1\n",
    "        except Exception as e : \n",
    "            return_warning_messages(e, step = step)\n",
    "            return 1\n",
    "\n",
    "        try : # STEP 3 : Modeling\n",
    "            prediction = NN_modeling(X_train, y_train, X_test, patience=100, total_epoch=5)\n",
    "            step += 1\n",
    "        except Exception as e : \n",
    "            return_warning_messages(e, step = step)\n",
    "            return 1\n",
    "\n",
    "        try : # STEP 4 : Save Prediction Data\n",
    "            submission_path = './submissions/DAE.csv'\n",
    "            save_prediction(sample, submission_path, prediction, y_train)\n",
    "        except Exception as e : \n",
    "            return_warning_messages(e, step = 0)\n",
    "            return 1\n",
    "\n",
    "        print('')\n",
    "        print('!! NN MAIN ENDED !!  /  ', datetime.today())\n",
    "\n",
    "    except Exception as e : \n",
    "        return_warning_messages(e, step = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0e2eabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!! NN MAIN STARTED !!  /   2021-12-16 20:18:47.575688\n",
      "2021-12-16 20:18:47.575688 STARTED\n",
      "\n",
      "=== UPDATE DATA ===\n",
      "=== NN Data Exist ===\n",
      "=== AUTO ENCODING STARTED ===  / 2021-12-16 20:18:48.750207\n",
      "(32000, 6704) (4640, 6704) 0.0025652787648141385\n",
      "=== AUTO ENCODING ENDED ===  / 2021-12-16 20:21:38.224983\n",
      "2021-12-16 20:21:40.113645 UPDATED\n",
      "\n",
      "Data Type : Numpy Array\n",
      "Mean Target Encoding -> Polynomial Features & Onehot Encoding -> Deep Auto Encoding\n",
      "(32000, 6749) (32000,) (4640, 6749)\n",
      "\n",
      "========== EPOCH : 0 ==========\n",
      "    ===== FOLD : 0 =====     ===== FOLD : 1 =====     ===== FOLD : 2 =====     ===== FOLD : 3 =====     ===== FOLD : 4 ===== \n",
      "     4.709329605102539        4.5456929206848145        4.493783950805664        4.832345962524414        4.24008321762085   \n",
      "\n",
      "========== EPOCH : 1 ==========\n",
      "    ===== FOLD : 0 =====     ===== FOLD : 1 =====     ===== FOLD : 2 =====     ===== FOLD : 3 =====     ===== FOLD : 4 ===== \n",
      "     4.746572971343994        4.689569473266602        4.3002729415893555        4.4574761390686035        4.388387680053711   \n",
      "\n",
      "========== EPOCH : 2 ==========\n",
      "    ===== FOLD : 0 =====     ===== FOLD : 1 =====     ===== FOLD : 2 =====     ===== FOLD : 3 =====     ===== FOLD : 4 ===== \n",
      "     4.71743106842041        4.48560905456543        4.301214694976807        4.210091590881348        5.101912975311279   \n",
      "\n",
      "========== EPOCH : 3 ==========\n",
      "    ===== FOLD : 0 =====     ===== FOLD : 1 =====     ===== FOLD : 2 =====     ===== FOLD : 3 =====     ===== FOLD : 4 ===== \n",
      "     4.471546173095703        5.065011024475098        4.632460117340088        4.339801788330078        4.271862030029297   \n",
      "\n",
      "========== EPOCH : 4 ==========\n",
      "    ===== FOLD : 0 =====     ===== FOLD : 1 =====     ===== FOLD : 2 =====     ===== FOLD : 3 =====     ===== FOLD : 4 ===== \n",
      "     4.919325828552246        4.685109615325928        4.00106143951416        4.924535751342773        4.3177714347839355   \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4640.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.719107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.458767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.739633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.893864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.781611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>165.709824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "count  4640.000000\n",
       "mean      4.719107\n",
       "std       4.458767\n",
       "min       3.000000\n",
       "25%       3.739633\n",
       "50%       3.893864\n",
       "75%       4.781611\n",
       "max     165.709824"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Prediction Saved At : ./submissions/DAE.csv ===\n",
      "\n",
      "!! NN MAIN ENDED !!  /   2021-12-16 21:31:13.064512\n"
     ]
    }
   ],
   "source": [
    "NN_data_path = './data/NN_DATA.pkl'\n",
    "need_update = True\n",
    "need_AE_update = True\n",
    "__NN_main__(NN_data_path, need_update, need_AE_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1076e55e",
   "metadata": {},
   "source": [
    "## 4. Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70b06f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pred = pd.read_csv('./submissions/CATBOOST.csv')['INVC_CONT'] # 5.3587728343\n",
    "dae_pred = pd.read_csv('./submissions/DAE.csv')['INVC_CONT'] # 5.3398171479\n",
    "# 위 두 submission의 correlationship이 낮아 두개만 ensemble\n",
    "\n",
    "final = cat_pred * 0.7 + dae_pred * 0.3\n",
    "sample = pd.read_csv('./data/sample_submission.csv')\n",
    "sample['INVC_CONT'] = final\n",
    "sample.to_csv('./submissions/JayHongPred.csv', index= False) # 5.226109375"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
